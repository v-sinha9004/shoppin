# ğŸ•·ï¸ Product URL Crawler

A scalable, asynchronous web crawler built using `Playwright`, `aiohttp`, and `BeautifulSoup` to extract product URLs from e-commerce websites that require JavaScript rendering.

---

## ğŸ“¦ Features

- âœ… Supports both static and JS-rendered pages (via Playwright)
- ğŸ” Detects product pages using:
  - URL pattern matching
  - `JSON-LD` structured data
  - `meta` tags like `og:type=product`
  - Keyword-based content scanning
- ğŸ’¾ Saves discovered URLs to CSV and persists metadata in DB
- ğŸ” Automatically crawls internal links with controlled depth
- ğŸ§  Domain-specific crawling behavior using a JSON config

---

## ğŸ› ï¸ Tech Stack

- **Python 3.8+**
- `aiohttp` â€” Async HTTP requests
- `Playwright` â€” Headless browser automation
- `BeautifulSoup` â€” HTML parsing
- `CSV`, `JSON`, `os`, `re` â€” Core utilities
- `asyncio` â€” Concurrency
- `pymongo` - Database

---

### ğŸ“¦ Install dependencies

```bash
pip install -r requirements.txt
playwright install
```

## ğŸ“ Project Structure

```
ProductCrawler/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ domains.json           # Domain-specific settings
â”œâ”€â”€ output/                    # CSVs for crawled results
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ constants.py           # Static values and keywords
â”‚   â”œâ”€â”€ config_loader.py       # Load & access domain config
â”‚   â”œâ”€â”€ helpers.py             # Link extraction, product detection, etc.
â”‚   â””â”€â”€ playwright_fetcher.py  # JS rendering using Playwright
â”œâ”€â”€ db.py                      # Function to persist scraped data
â”œâ”€â”€ main.py                    # Entry point
â””â”€â”€ README.md                  # This file
```

---

## âš™ï¸ Config File: `config/domains.json`

Define each domainâ€™s behavior:

```json
{
  "westside.com": {
    "requires_js": true,
    "product_url_patterns": ["/product/"]
  },
  "nykaafashion.com": {
    "requires_js": true,
    "product_url_patterns": ["/p/", "/product/"]
  }
}
```

---

## â–¶ï¸ Usage

```bash
python main.py
```

This will crawl each domain in `config/domains.json`, find product URLs, and save them in:

```
output/<domain>.csv
```

Each row contains:

```
[reason_matched, product_url]
```

Example:

```
JSON-LD @type: Product, https://www.example.com/product/shoes123
```

---

## ğŸ’¾ Saving to DB

`db.py` provides a `save_scraped_data(url, domain_key, is_product, reason)` function.

You can integrate this with your own DB system (e.g., PostgreSQL, MySQL, MongoDB).

---

## ğŸ› ï¸ Production Considerations

To scale and run this crawler in a production environment, keep in mind:

### âœ… Headless Environment Setup

- Install **screen** or **tmux** so you can run long crawling jobs persistently:
  ```bash
  sudo apt install screen
  screen -S crawler-session
  ```
  Then inside screen:
  ```bash
  python main.py
  ```
  Detach: `Ctrl+A D`, Reattach later: `screen -r crawler-session`

### âœ… Storage & Search

When data grows to **millions of records**, consider:

- Use **Elasticsearch** to store and search through product URLs efficiently.
- Replace `save_scraped_data` with a function that indexes into Elasticsearch.

### âœ… Config Management

- Use **external config servers** (e.g., AWS Parameter Store, Vault) in place of hardcoded JSON files.
- Use **environment variables** for things like DB credentials, headless mode toggles, etc.

### âœ… Monitoring & Retry Logic

- Add retry decorators (`tenacity`, custom retry logic) for transient failures.
- Push logs to **Grafana + Loki** or **ELK stack** for real-time monitoring.
- Use queues like **RabbitMQ** or **Kafka** if scaling to multiple workers.

### âœ… Dockerization

- Containerize the app with `Docker` and orchestrate with `Docker Compose` or `Kubernetes`.

### âš™ï¸ Scaling in Production

- Use Celery with Redis/RabbitMQ to distribute crawling tasks across multiple workers.
- Containerize with Docker and deploy using Kubernetes for horizontal scalability.
- For thousands of domains, use a producer-consumer model with message queues (e.g., Kafka).

---

## ğŸ“Œ Notes

- Playwright headless mode is automatically disabled for some sites (like Nykaa) to ensure proper rendering.
- Uses multiple product identification techniques for improved accuracy.
- Crawling depth is limited (default: 3) to avoid infinite loops or deep crawling.

---

## ğŸ§‘â€ğŸ’» Author

**Vishal Sinha**  
Freelance Full Stack Developer | [LinkedIn](https://www.linkedin.com/in/vishal-sinha-dev)

---

## ğŸ“ License

MIT License @ Vishal Sinha
